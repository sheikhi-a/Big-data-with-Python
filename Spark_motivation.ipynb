{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheikhi-a/Big-data-with-Python/blob/main/Spark_motivation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Do we need to use spark instead of pandas and numpy?\n",
        "\n",
        "**By Ayyub  Sheikhi**\n",
        "\n",
        "Department of statistics,\n",
        "Shahid Bahonar University of Kerman\n"
      ],
      "metadata": {
        "id": "MiZ0WVQ_YbPW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuS1mwn8LJoe"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cl8tyepQK8ud"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[1]\") \\\n",
        "    .appName(\"SparkByExamples.com\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "print(spark)\n",
        "spark\n",
        "\n"
      ],
      "metadata": {
        "id": "kufF37QLUA6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following codes  by changing the sample size\n",
        "\n",
        "Forst: small sample size (ARRAY_SIZE = 10)"
      ],
      "metadata": {
        "id": "r2mKJDGHWz_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "ARRAY_SIZE = 10\n",
        "#data = np.random.rand(ARRAY_SIZE)\n",
        "pandas_df = pd.DataFrame({'A': np.random.rand(ARRAY_SIZE),\n",
        "                          'B': np.random.rand(ARRAY_SIZE),\n",
        "                          'C': np.random.rand(ARRAY_SIZE)})\n",
        "pandas_df\n",
        "pandas_df.head(5)\n",
        "\n",
        "# Create a PySpark dataframe\n",
        "pyspark_df = spark.createDataFrame(pandas_df)\n",
        "pyspark_df.head(5)\n",
        "\n",
        "# Time the pandas operation\n",
        "pandas_start_time = time.time()\n",
        "pandas_df['D'] = pandas_df['A'] * pandas_df['B'] ** pandas_df['C']\n",
        "pandas_end_time = time.time()\n",
        "pandas_time = pandas_end_time - pandas_start_time\n",
        "print(\"Time taken for pandas operation: {} seconds\".format(pandas_time))\n",
        "\n",
        "\n",
        "# Time the PySpark operation\n",
        "pyspark_start_time = time.time()\n",
        "pyspark_df = pyspark_df.withColumn('D', pyspark_df.A * pyspark_df.B ** pyspark_df.C)\n",
        "pyspark_end_time = time.time()\n",
        "pyspark_time = pyspark_end_time - pyspark_start_time\n",
        "\n",
        "print(\"Time taken for PySpark operation: {} seconds\".format(pyspark_time))"
      ],
      "metadata": {
        "id": "7rLedQkOK3KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the agian with more sample size (ARRAY_SIZE = 1000)\n"
      ],
      "metadata": {
        "id": "PLS5PK1mXcnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "ARRAY_SIZE = 1000\n",
        "#data = np.random.rand(ARRAY_SIZE)\n",
        "pandas_df = pd.DataFrame({'A': np.random.rand(ARRAY_SIZE),\n",
        "                          'B': np.random.rand(ARRAY_SIZE),\n",
        "                          'C': np.random.rand(ARRAY_SIZE)})\n",
        "pandas_df\n",
        "pandas_df.head(5)\n",
        "\n",
        "# Create a PySpark dataframe\n",
        "pyspark_df = spark.createDataFrame(pandas_df)\n",
        "pyspark_df.head(5)\n",
        "\n",
        "# Time the pandas operation\n",
        "pandas_start_time = time.time()\n",
        "pandas_df['D'] = pandas_df['A'] * pandas_df['B'] ** pandas_df['C']\n",
        "pandas_end_time = time.time()\n",
        "pandas_time = pandas_end_time - pandas_start_time\n",
        "print(\"Time taken for pandas operation: {} seconds\".format(pandas_time))\n",
        "\n",
        "\n",
        "# Time the PySpark operation\n",
        "pyspark_start_time = time.time()\n",
        "pyspark_df = pyspark_df.withColumn('D', pyspark_df.A * pyspark_df.B ** pyspark_df.C)\n",
        "pyspark_end_time = time.time()\n",
        "pyspark_time = pyspark_end_time - pyspark_start_time\n",
        "\n",
        "print(\"Time taken for PySpark operation: {} seconds\".format(pyspark_time))"
      ],
      "metadata": {
        "id": "y6b5vMeRXuoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run again with more size (10000)"
      ],
      "metadata": {
        "id": "JhZDt1p3Xt5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aiT6M2CuX8YF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "ARRAY_SIZE = 10000\n",
        "#data = np.random.rand(ARRAY_SIZE)\n",
        "pandas_df = pd.DataFrame({'A': np.random.rand(ARRAY_SIZE),\n",
        "                          'B': np.random.rand(ARRAY_SIZE),\n",
        "                          'C': np.random.rand(ARRAY_SIZE)})\n",
        "pandas_df\n",
        "pandas_df.head(5)\n",
        "\n",
        "# Create a PySpark dataframe\n",
        "pyspark_df = spark.createDataFrame(pandas_df)\n",
        "pyspark_df.head(5)\n",
        "\n",
        "# Time the pandas operation\n",
        "pandas_start_time = time.time()\n",
        "pandas_df['D'] = pandas_df['A'] * pandas_df['B'] ** pandas_df['C']\n",
        "pandas_end_time = time.time()\n",
        "pandas_time = pandas_end_time - pandas_start_time\n",
        "print(\"Time taken for pandas operation: {} seconds\".format(pandas_time))\n",
        "\n",
        "\n",
        "# Time the PySpark operation\n",
        "pyspark_start_time = time.time()\n",
        "pyspark_df = pyspark_df.withColumn('D', pyspark_df.A * pyspark_df.B ** pyspark_df.C)\n",
        "pyspark_end_time = time.time()\n",
        "pyspark_time = pyspark_end_time - pyspark_start_time\n",
        "\n",
        "print(\"Time taken for PySpark operation: {} seconds\".format(pyspark_time))"
      ],
      "metadata": {
        "id": "oRj5rFukX9fL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even More (1000000)"
      ],
      "metadata": {
        "id": "dRnDiNGdYBef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "ARRAY_SIZE = 1000000\n",
        "#data = np.random.rand(ARRAY_SIZE)\n",
        "pandas_df = pd.DataFrame({'A': np.random.rand(ARRAY_SIZE),\n",
        "                          'B': np.random.rand(ARRAY_SIZE),\n",
        "                          'C': np.random.rand(ARRAY_SIZE)})\n",
        "pandas_df\n",
        "pandas_df.head(5)\n",
        "\n",
        "# Create a PySpark dataframe\n",
        "pyspark_df = spark.createDataFrame(pandas_df)\n",
        "pyspark_df.head(5)\n",
        "\n",
        "# Time the pandas operation\n",
        "pandas_start_time = time.time()\n",
        "pandas_df['D'] = pandas_df['A'] * pandas_df['B'] ** pandas_df['C']\n",
        "pandas_end_time = time.time()\n",
        "pandas_time = pandas_end_time - pandas_start_time\n",
        "print(\"Time taken for pandas operation: {} seconds\".format(pandas_time))\n",
        "\n",
        "\n",
        "# Time the PySpark operation\n",
        "pyspark_start_time = time.time()\n",
        "pyspark_df = pyspark_df.withColumn('D', pyspark_df.A * pyspark_df.B ** pyspark_df.C)\n",
        "pyspark_end_time = time.time()\n",
        "pyspark_time = pyspark_end_time - pyspark_start_time\n",
        "\n",
        "print(\"Time taken for PySpark operation: {} seconds\".format(pyspark_time))"
      ],
      "metadata": {
        "id": "gZz_dXs0YFiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do the above procedure for matrix inversion\n",
        "\n",
        "\n",
        "\n",
        "Change the matrix size\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BxvNtg_SYId-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFd6nz-HLEid"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Matrix Inversion: PySpark vs NumPy\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Generate a random square matrix\n",
        "MATRIX_SIZE = 3  # Adjust for larger matrices\n",
        "matrix = np.random.rand(MATRIX_SIZE, MATRIX_SIZE)\n",
        "\n",
        "# Ensure the matrix is invertible by adding a diagonal dominance\n",
        "matrix += MATRIX_SIZE * np.eye(MATRIX_SIZE)\n",
        "\n",
        "# NumPy Implementation\n",
        "start_time = time.time()\n",
        "numpy_inverse = np.linalg.inv(matrix)\n",
        "numpy_time = time.time() - start_time\n",
        "print(f\"NumPy time: {numpy_time:.4f} seconds\")\n",
        "\n",
        "# PySpark Implementation\n",
        "# Split matrix rows into an RDD\n",
        "rdd_matrix = spark.sparkContext.parallelize(matrix.tolist())\n",
        "\n",
        "# Function to compute the inverse using distributed row-wise processing\n",
        "def compute_inverse(matrix):\n",
        "    # Convert the matrix into a NumPy array\n",
        "    np_matrix = np.array(matrix)\n",
        "    return np.linalg.inv(np_matrix).tolist()\n",
        "\n",
        "start_time = time.time()\n",
        "# Compute the matrix inverse (this step does not benefit from distributed computation)\n",
        "pyspark_inverse = compute_inverse(rdd_matrix.collect())  # Collect entire matrix for inversion\n",
        "pyspark_time = time.time() - start_time\n",
        "print(f\"PySpark time: {pyspark_time:.4f} seconds\")\n",
        "\n",
        "# Compare Results\n",
        "if np.allclose(numpy_inverse, pyspark_inverse, atol=1e-6):\n",
        "    print(\"Results match!\")\n",
        "else:\n",
        "    print(\"Results do not match!\")\n",
        "\n",
        "# Close the Spark session\n",
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pAQCmd36XSEt"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgMdL3mRR2dxdv+oXHzkjO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}